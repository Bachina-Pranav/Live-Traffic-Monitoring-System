{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "import json\n",
    "from asyncio import Semaphore\n",
    "import time\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from jupyter_dash import JupyterDash  # Instead of Dash, use JupyterDash\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "url = \"https://onemotoring.lta.gov.sg/content/onemotoring/home/driving/traffic_information/traffic-cameras.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicScraper:\n",
    "    def __init__(self, num_parallel_requests, data_path):\n",
    "        self.num_parallel_requests = num_parallel_requests\n",
    "        self.semaphore = Semaphore(num_parallel_requests)\n",
    "        self.main_page_mapping = {}\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def _parse_main_page(self, soup):\n",
    "        # Parse the main page and extract camera names and URLs\n",
    "        scroll_div = soup.find(\"div\", {\"id\": \"scroll\"})\n",
    "        buttons = scroll_div.find_all(\"button\")\n",
    "        if not buttons:\n",
    "            raise Exception(\"No buttons found in the main page\")\n",
    "        for button in buttons:\n",
    "            camera_name = button[\"id\"]\n",
    "            camera_url = url.split(\".html\")[0] + \"/\" + camera_name + \".html#trafficCameras\"\n",
    "            self.main_page_mapping[camera_name] = camera_url\n",
    "\n",
    "    async def _fetch(self, session, url):\n",
    "        async with self.semaphore:\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    if response.status == 404:\n",
    "                        print(f\"Error 404: {url} not found.\")\n",
    "                        return None\n",
    "                    response.raise_for_status()\n",
    "                    return await response.text()\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"Request failed for {url}: {e}\")\n",
    "                return None\n",
    "\n",
    "    async def _main_page_scraper(self):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            html_content = await self._fetch(session, url)\n",
    "            if html_content:\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                self._parse_main_page(soup)\n",
    "\n",
    "    async def _camera_page_scraper(self, session, camera_url, directory):\n",
    "        previous_metadata = {}\n",
    "        if os.path.exists(f\"{directory}/metadata.json\"):\n",
    "            with open(f\"{directory}/metadata.json\", \"r\") as f:\n",
    "                previous_metadata = json.load(f)\n",
    "        # print(f\"Scraping {camera_url}\")\n",
    "        metadata = {}\n",
    "        html_content = await self._fetch(session, camera_url)\n",
    "        if not html_content:\n",
    "            return None\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        image_container = soup.find(\"div\", {\"class\": \"snapshots\"})\n",
    "        if not image_container:\n",
    "            # print(f\"No snapshots found on {camera_url}\")\n",
    "            return None\n",
    "        header = image_container.find_all(\"h2\", {\"id\": \"expressway-name\"})\n",
    "        if not header:\n",
    "            # print(f\"No expressway name found on {camera_url}\")\n",
    "            return None\n",
    "        metadata[\"camera\"] = header[0].text\n",
    "        metadata_entry = {}\n",
    "        cards = soup.find(\"div\", {\"class\": \"road-snapshots\"})\n",
    "        if not cards:\n",
    "            # print(f\"No road snapshots found on {camera_url}\")\n",
    "            return None\n",
    "        for card in cards.find_all(\"div\", {\"class\": \"card\"}):\n",
    "            image = card.find(\"img\")\n",
    "            timestamp = card.find(\"div\", {\"class\": \"timestamp\"})\n",
    "            if timestamp:\n",
    "                # In the span tag, the text is the timestamp\n",
    "                timestamp = timestamp.find(\"span\").text\n",
    "                timestamp = timestamp.split(\" \")[3]\n",
    "            if not image:\n",
    "                # print(f\"No image found in card on {camera_url}\")\n",
    "                continue\n",
    "            if previous_metadata and image[\"alt\"] in previous_metadata.get(\"images\", {}):\n",
    "                # Check if there is a match between the timestamps\n",
    "                if previous_metadata[\"images\"][image[\"alt\"]][1] == timestamp:\n",
    "                    # Skip downloading the image if it already exists\n",
    "                    metadata_entry[image[\"alt\"]] = [image[\"src\"], timestamp, False]\n",
    "                    continue\n",
    "                else:\n",
    "                    # Timestamp changes\n",
    "                    print(\"For image\", image[\"alt\"], \"Timestamp changed from\", previous_metadata[\"images\"][image[\"alt\"]][1], \"to\", timestamp)\n",
    "            metadata_entry[image[\"alt\"]] = [image[\"src\"], timestamp, True]\n",
    "        metadata[\"images\"] = metadata_entry\n",
    "        # Write the metadata to a file\n",
    "        with open(f\"{directory}/metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata, f)\n",
    "        return metadata\n",
    "\n",
    "    async def _download_image(self, session, image_url, path, alt, download):\n",
    "        safe_filename = \"_\".join(alt.replace(\"/\", \"_\").split(\" \"))\n",
    "        if not download:\n",
    "            pass\n",
    "        async with self.semaphore:\n",
    "            try:\n",
    "                async with session.get(\"https:\" + image_url) as response:\n",
    "                    if response.status == 404:\n",
    "                        # print(f\"Image not found: {image_url}\")\n",
    "                        return\n",
    "                    response.raise_for_status()\n",
    "                    content = await response.read()\n",
    "                    with open(f\"{path}/{safe_filename}.jpg\", \"wb\") as f:\n",
    "                        f.write(content)\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"Failed to download {image_url}: {e}\")\n",
    "\n",
    "    async def download_images(self, session, mapping, path):\n",
    "        if not mapping or \"images\" not in mapping:\n",
    "            # print(\"No valid images found.\")\n",
    "            return\n",
    "        tasks = [self._download_image(session, image_url, path, alt, download) for alt, (image_url, timestamp, download) in mapping['images'].items()]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    async def scrape(self):\n",
    "        await self._main_page_scraper()\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = []\n",
    "            for camera_name, camera_url in self.main_page_mapping.items():\n",
    "                camera_directory = f\"{self.data_path}/{camera_name}\"\n",
    "                os.makedirs(camera_directory, exist_ok=True)\n",
    "                camera_data = await self._camera_page_scraper(session, camera_url, camera_directory)\n",
    "                if camera_data:\n",
    "                    # print(\"Entered!\")\n",
    "                    task = self.download_images(session, camera_data, camera_directory)\n",
    "                    tasks.append(task)\n",
    "                else:\n",
    "                    print(\"Skipping camera\", camera_name)\n",
    "            await asyncio.gather(*tasks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_and_get_stats():\n",
    "    directory_path = 'data'  # Replace with the directory path\n",
    "    directories = [os.path.join(directory_path, d) for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "\n",
    "    assets_directory = 'assets'  # Directory where Dash serves static files\n",
    "\n",
    "    # Clear previous processed images from assets folder\n",
    "    if os.path.exists(assets_directory):\n",
    "        shutil.rmtree(assets_directory)  # Delete the folder\n",
    "    os.makedirs(assets_directory, exist_ok=True)  # Recreate assets directory\n",
    "\n",
    "    camera_data = []\n",
    "    model = YOLO('yolov10x.pt')  # You can use yolov8s.pt or yolov8m.pt for better accuracy\n",
    "\n",
    "    # Define the vehicle classes according to COCO dataset used by YOLO\n",
    "    vehicle_classes = {\n",
    "        'car': 2,\n",
    "        'truck': 7,\n",
    "        'bus': 5,\n",
    "        'motorbike': 3\n",
    "    }\n",
    "\n",
    "    # Loop through each directory and process the images\n",
    "    for directory in tqdm(directories, desc='Processing images'):\n",
    "        image_directory = directory\n",
    "        car_count, truck_count, bus_count, motorbike_count = 0, 0, 0, 0\n",
    "\n",
    "        # List all image files in the directory\n",
    "        image_files = [f for f in os.listdir(image_directory) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
    "        if image_files:\n",
    "            file_path = f'{directory}/metadata.json'\n",
    "            # Open and read the JSON file to get camera name\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            camera_name = data.get('camera', 'Unknown Camera')\n",
    "            times = {view: details[1] for view, details in data[\"images\"].items()}\n",
    "\n",
    "            # Process each image\n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(image_directory, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                # Run YOLOv8 inference -> without verbosity\n",
    "                results = model(image)\n",
    "\n",
    "                # Draw bounding boxes and classify objects\n",
    "                for box in results[0].boxes:\n",
    "                    # Extract box details\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    class_id = int(box.cls[0].cpu().numpy())\n",
    "                    confidence = box.conf[0].cpu().numpy()\n",
    "                    \n",
    "                    # Check if the detected object is a vehicle\n",
    "                    if class_id == vehicle_classes['car']:\n",
    "                        color = (0, 255, 0)  # Green for cars\n",
    "                        car_count += 1\n",
    "                    elif class_id == vehicle_classes['truck']:\n",
    "                        color = (0, 0, 255)  # Red for trucks\n",
    "                        truck_count += 1\n",
    "                    elif class_id == vehicle_classes['bus']:\n",
    "                        color = (255, 0, 0)  # Blue for buses\n",
    "                        bus_count += 1\n",
    "                    elif class_id == vehicle_classes['motorbike']:\n",
    "                        color = (255, 255, 0)  # Yellow for motorbikes\n",
    "                        motorbike_count += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Draw the bounding box on the image\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "                    label = f\"{list(vehicle_classes.keys())[list(vehicle_classes.values()).index(class_id)]} {confidence:.2f}\"\n",
    "                    cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "                # Save the processed image in the assets folder\n",
    "                processed_image_path = os.path.join(assets_directory, f\"processed_{camera_name}_{image_file}\")\n",
    "                cv2.imwrite(processed_image_path, image)\n",
    "\n",
    "            # Append the data for this camera\n",
    "            camera_data.append({\n",
    "                'camera_name': camera_name,\n",
    "                'car_count': car_count,\n",
    "                'truck_count': truck_count,\n",
    "                'bus_count': bus_count,\n",
    "                'motorbike_count': motorbike_count,\n",
    "                'image_files': [f\"processed_{camera_name}_{image_file}\" for image_file in image_files],  # List of processed images\n",
    "                'times': times\n",
    "            })\n",
    "    \n",
    "    return camera_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def periodic_scrape(scraper):\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        await scraper.scrape()\n",
    "        print(f\"Scraping completed in {time.time() - start_time} seconds\")\n",
    "        await asyncio.sleep(60)  # Wait for 1 minute before next scrape\n",
    "\n",
    "# Set up JupyterDash app\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "# Dash layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Vehicle Counts Dashboard\"),\n",
    "    html.Div(id='vehicle-counts-text'),\n",
    "    dcc.Interval(\n",
    "        id='interval-component',\n",
    "        interval=10*1000,  # Update every ten seconds\n",
    "        n_intervals=0\n",
    "    )\n",
    "])\n",
    "\n",
    "# Callback to update the text with vehicle counts\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('vehicle-counts-text', 'children'),\n",
    "    [dash.dependencies.Input('interval-component', 'n_intervals')]\n",
    ")\n",
    "def update_text(n_intervals):\n",
    "    camera_data = process_images_and_get_stats()\n",
    "\n",
    "    # Calculate total counts for each vehicle type\n",
    "    total_cars = sum(data['car_count'] for data in camera_data)\n",
    "    total_trucks = sum(data['truck_count'] for data in camera_data)\n",
    "    total_buses = sum(data['bus_count'] for data in camera_data)\n",
    "    total_motorbikes = sum(data['motorbike_count'] for data in camera_data)\n",
    "\n",
    "    # Create a bar chart for total vehicle counts\n",
    "    vehicle_counts = {\n",
    "        'Vehicle Type': ['Cars', 'Trucks', 'Buses', 'Motorbikes'],\n",
    "        'Count': [total_cars, total_trucks, total_buses, total_motorbikes]\n",
    "    }\n",
    "    \n",
    "    # Generate a bar chart using Plotly Express\n",
    "    fig = px.bar(vehicle_counts, x='Vehicle Type', y='Count', title='Total Vehicle Counts')\n",
    "\n",
    "    # Prepare the layout for each camera, including images and counts\n",
    "    text_elements = []\n",
    "    for data in camera_data:\n",
    "        image_elements = []\n",
    "        camera_name = data['camera_name']\n",
    "        \n",
    "        for image_file in data['image_files']:\n",
    "            # Generate a timestamp to append as a query string for the image URL\n",
    "            cleaned_name = image_file.replace(f\"processed_{camera_name}_\", \"\").replace(\".jpg\", \"\")\n",
    "            cleaned_name = cleaned_name.replace(\"_\",\" \")\n",
    "            timestamp = int(time.time())  # Current time in seconds\n",
    "            image_url = f\"/assets/{image_file}?v={timestamp}\"  # Append timestamp to force reload\n",
    "\n",
    "            # Wrap image and its cleaned name together\n",
    "            image_elements.append(html.Div([\n",
    "                # Display the image\n",
    "                html.Img(\n",
    "                    src=image_url,  # Use the new URL with timestamp\n",
    "                    style={'width': '300px', 'height': 'auto', 'padding': '10px'}\n",
    "                ),\n",
    "                # Display the cleaned image name\n",
    "                html.P(cleaned_name, style={'text-align': 'center', 'font-size': '12px'})\n",
    "            ], style={'display': 'inline-block', 'text-align': 'center'}))\n",
    "\n",
    "        \n",
    "        text_elements.append(\n",
    "            html.Div([\n",
    "                html.H3(f\"Camera: {camera_name}\"),\n",
    "                html.P(f\"Cars: {data['car_count']}\"),\n",
    "                html.P(f\"Trucks: {data['truck_count']}\"),\n",
    "                html.P(f\"Buses: {data['bus_count']}\"),\n",
    "                html.P(f\"Motorbikes: {data['motorbike_count']}\"),\n",
    "                html.Div(image_elements, style={'display': 'flex', 'flex-wrap': 'wrap'}),  # Display images side by side\n",
    "                html.Hr()  # Horizontal line separator\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Combine the graph and the camera data elements\n",
    "    return [\n",
    "        dcc.Graph(figure=fig),  # Insert the graph at the top\n",
    "        *text_elements  # Followed by the detailed camera information\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# Run the JupyterDash app\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PeriodicScraper(50, \"data\")\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.create_task(periodic_scrape(scraper))  # Start periodic scraping\n",
    "    app.run(jupyter_mode=\"tab\") # Run the dashboard in Jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
